{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Gizmodo: 200\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5e17010b2788>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m     \u001b[0mgizmo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0meuropaPress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0mlaVanguardia\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-5e17010b2788>\u001b[0m in \u001b[0;36mgizmo\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0mfecha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'sc-3nbvzd-1 kpXIm'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0;31m#IMPRIME EN LINK DE LA IMAGEN DEL ARTICULO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0mimagen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'img'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'data-format'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'jpg'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'srcset'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m                 gizmodo['Articles'].append({\n\u001b[1;32m     81\u001b[0m                     \u001b[0;34m'Redir'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcant_redir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import threading\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "#driver = webdriver.Chrome(\"./chromedriver.exe\")\n",
    "#driver.get(\"https://www.critica.com.pa/mundo\")\n",
    "gizmodo = {} #DICCIONARIO\n",
    "gizmodo['Articles'] = []\n",
    "europa_press = {}\n",
    "europa_press['Articles'] = []\n",
    "vanguardia = {} #DICCIONARIO\n",
    "vanguardia['Articles'] = []\n",
    "\n",
    "URL_ALMIN = 'https://www.lavanguardia.com/alminuto'\n",
    "URL = 'https://es.gizmodo.com/c/tecnologia'\n",
    "URL_EUROPA = 'https://www.europapress.es/internacional/'\n",
    "        \n",
    "def content_page(link):\n",
    "    link_cont = requests.get(link)\n",
    "    soup = BeautifulSoup(link_cont.text, 'html.parser')\n",
    "    container = soup.find_all('p',{'class':'sc-77igqf-0 dQFbxq'})\n",
    "    parraf = []\n",
    "    for parrafo in container:\n",
    "        parrafo\n",
    "        parraf.append(parrafo.getText())\n",
    "    return(parraf)\n",
    "    #return parraf\n",
    "'''\n",
    "class Scrapy:\n",
    "    def __ini__(self,title,author,content,date,urlLink,cont_link,img):\n",
    "        self.title = title\n",
    "        self.author = author\n",
    "        self.content = content\n",
    "        self.date = date\n",
    "        self.urlLink = urlLink\n",
    "        self.cont_link = cont_link\n",
    "        self.img = img\n",
    "        \n",
    "    def __str__(self):\n",
    "        return str(self.__class__) + \": \" + str(self.__dict__)\n",
    "'''\n",
    "    \n",
    "\n",
    "\n",
    "            \n",
    "def gizmo():\n",
    "    re = requests.get(URL)\n",
    "    codeg = re.status_code\n",
    "    print(f'Status Gizmodo: {codeg}')\n",
    "    if re.status_code == 200:\n",
    "        soup = BeautifulSoup(re.text, 'html.parser')\n",
    "\n",
    "        if soup is not None:\n",
    "            articles = soup.find_all('article',{'class':'cw4lnv-0 iTueKC js_post_item'})\n",
    "            #redires = soup.find_all\n",
    "            cant_redir = len(soup.find_all('a')) #REDIRECCIONES\n",
    "            #noticias['Articles'].append({'Redir':cant_redir})\n",
    "            for article in articles:\n",
    "                #IMPRIME EL TITULO DEL ARTICULO\n",
    "                titulo = article.find('h2', {'class':'sc-759qgu-0 iSjOfj cw4lnv-6 fGbMFF'}).getText()\n",
    "                #IMPRIME EL AUTOR DEL ARTICULO\n",
    "                autor = article.find('div',{'class':'sc-1mep9y1-0 sc-1ixdk2y-0 fCQpxO sc-1rye1-3 hwANEt'}).text\n",
    "                #IMPRIME EL LINK DEL ARTICULO\n",
    "                link = article.find('a',{'class':'sc-1out364-0 hMndXN js_link'}).get('href')\n",
    "                #IMPRIME EL CONTENIDO DEL ARTICULO\n",
    "                article_content = content_page(link)\n",
    "                #IMPRIME LA FECHA DEL ARTICULO\n",
    "                fecha = article.find('div',{'class':'sc-3nbvzd-1 kpXIm'}).getText()\n",
    "                #IMPRIME EN LINK DE LA IMAGEN DEL ARTICULO\n",
    "                imagen = article.find('img',{'data-format':'jpg'}).get('srcset')\n",
    "                gizmodo['Articles'].append({\n",
    "                    'Redir':cant_redir,\n",
    "                    'Title':titulo,\n",
    "                    'Author':autor,\n",
    "                    'Link':link,\n",
    "                    'Content':article_content,\n",
    "                    'Date':fecha,\n",
    "                    'Img':imagen\n",
    "                })\n",
    "\n",
    "            #print(gizmodo)\n",
    "            with open('gizmodo.json','w') as f:\n",
    "                json.dump(gizmodo, f, indent=4)\n",
    "                               \n",
    "    print('Archivo de gizmodo listo')\n",
    "\n",
    "    \n",
    "def europaPress():\n",
    "    re = requests.get(URL_EUROPA)\n",
    "    codee = re.status_code\n",
    "    print(f'Status Europapress: {codee}')\n",
    "    if re.status_code == 200:\n",
    "        soup = BeautifulSoup(re.text, 'html.parser')\n",
    "\n",
    "        if soup is not None:\n",
    "            mundo_articles = soup.find_all('article',{'class':'articulo-portada'})\n",
    "            \n",
    "            #redires = soup.find_all\n",
    "            mundo_redir = len(soup.find_all('a')) #REDIRECCIONES\n",
    "            #print(mundo_redir)\n",
    "            #noticias['Articles'].append({'Redir':cant_redir})\n",
    "            for article in mundo_articles:\n",
    "                mundo_title = article.find('h2',{'class':'articulo-titulo'}).getText()\n",
    "                #print(mundo_title)\n",
    "                mundo_fecha = article.find('time',{'class':'hora_hace'}).getText()\n",
    "                #print(mundo_fecha)\n",
    "                mundo_link = article.find('a').get('href')\n",
    "                #print(mundo_link)\n",
    "                \n",
    "                link_req = requests.get(mundo_link)\n",
    "                soup = BeautifulSoup(link_req.text, 'html.parser')\n",
    "                mundo_container = soup.find_all('div',{'id':'NoticiaPrincipal'})\n",
    "                for mundo_art in mundo_container:\n",
    "                    #mundo_autor = mundo_art.find('span',{'class':'fotoPie'})\n",
    "                    mundo_autor = mundo_art.find('div',{'class':'captionv2'})\n",
    "                    if(hasattr(mundo_autor,'text')):\n",
    "                        mundo_autor = mundo_art.find('div',{'class':'captionv2'}).text\n",
    "                        mundo_autor = mundo_autor.split(\" - \")\n",
    "                        mundo_autor = mundo_autor[1]\n",
    "                        #print(f'Author: {mundo_autor}')\n",
    "                    else:\n",
    "                        mundo_autor = mundo_art.find('div',{'class':'captionv2'})\n",
    "                        #mundo_autor = mundo_autor.split(\" - \")\n",
    "                        #print(f'Author: {mundo_autor}')\n",
    "                    mundo_imagen = mundo_art.find('img',{'id':'fotoPrincipalNoticia'})\n",
    "                    if(hasattr(mundo_imagen,'get')):\n",
    "                        mundo_imagen = mundo_art.find('img',{'id':'fotoPrincipalNoticia'}).get('src')\n",
    "                        #print(f'Imagen: {mundo_imagen}')\n",
    "                    else:\n",
    "                        mundo_imagen = mundo_art.find('img',{'id':'fotoPrincipalNoticia'})\n",
    "                        #print(f'Imagen: {mundo_imagen}')\n",
    "                        \n",
    "                    mundo_parrafos = soup.find_all('div',{'id':'CuerpoNoticiav2'})\n",
    "                    mundo_parraf = []\n",
    "                    for parra in mundo_parrafos:\n",
    "                        mundo_parraf.append(parra.getText())\n",
    "                        #print(f'Contenido: {mundo_parraf}')\n",
    "           \n",
    "                europa_press['Articles'].append({\n",
    "                    'Redir':mundo_redir,\n",
    "                    'Title':mundo_title,\n",
    "                    'Author':mundo_autor,\n",
    "                    'Link':mundo_link,\n",
    "                    'Content':mundo_parraf,\n",
    "                    'Date':mundo_fecha,\n",
    "                    'Img':mundo_imagen\n",
    "                })\n",
    "                \n",
    "                #print(europa_press)\n",
    "                \n",
    "                with open('europapress.json','w') as f:\n",
    "                    json.dump(europa_press, f, indent=4)\n",
    "    print('Archivo de europapress listo')\n",
    "                \n",
    "def laVanguardia():\n",
    "    re = requests.get(URL_ALMIN)\n",
    "    codev = re.status_code\n",
    "    print(f'Status LaVanguardia: {codev}')\n",
    "    if re.status_code == 200:\n",
    "        soup = BeautifulSoup(re.text, 'html.parser')\n",
    "\n",
    "        if soup is not None:\n",
    "            van_articles = soup.find_all('li',{'class':'results-item'})\n",
    "            \n",
    "            #redires = soup.find_all\n",
    "            van_redir = len(soup.find_all('a')) #REDIRECCIONES\n",
    "            #print(van_redir)\n",
    "            #noticias['Articles'].append({'Redir':cant_redir})\n",
    "            for van_article in van_articles:\n",
    "                van_title = van_article.find('h1',{'class':'story-header-title'}).getText()\n",
    "                #print(van_title)\n",
    "                van_link = van_article.find('a',{'class':'story-header-title-link'}).get('href')\n",
    "                #print(van_link)\n",
    "                #TOMA EL LINK DEL POST EN MINIATURA PARA HACER UN NUEVO REQUEST, PERO ESTA VEZ DESDE DENTRO DEL POST\n",
    "                link_vanreq = requests.get(van_link)\n",
    "                soup = BeautifulSoup(link_vanreq.text, 'html.parser')\n",
    "                van_container = soup.find_all('div',{'class':'detail__main'})\n",
    "                for van_art in van_container:\n",
    "                    #CAPTURA EL AUTOR SI ESTA DISPONIBLE LA ETIQUETA\n",
    "                    if(van_art.find('figcaption',{'class':'img-figcaption'})):\n",
    "                        van_autor = van_art.find('figcaption',{'class':'img-figcaption'}).text\n",
    "                        #print(f'Author: {van_autor}')\n",
    "                    else:\n",
    "                        van_autor = 'Sin autor'\n",
    "                        #print(f'Author: {van_autor}')\n",
    "                    #CAPTURA LA FECHA\n",
    "                    van_fecha = van_art.find('time',{'class':'d-signature__time'}).text\n",
    "                    #print(f'Fecha: {van_fecha}')\n",
    "                    #sI EXISTE LA ETIQUETA DE IMAGEN LA CAPTURA\n",
    "                    if(van_art.find('img',{'class':'img-responsive'})):\n",
    "                        van_imagen = van_art.find('img',{'class':'img-responsive'}).get('src')\n",
    "                        #print(f'Imagen: {van_imagen}')\n",
    "                    else:\n",
    "                        van_imagen = 'Imagen no disponible'\n",
    "                        #print(f'Imagen: {van_imagen}')\n",
    "                    #BUSCA EL CONTENEDOR PADRE DE LOS PARRAFOS Y CAPTURA CADA PARRAFO\n",
    "                    van_parrafos = soup.find_all('div',{'itemprop':'articleBody'})\n",
    "                    van_parraf = []\n",
    "                    for van_parra in van_parrafos:\n",
    "                        van_parraf.append(van_parra.getText())\n",
    "                        #print(f'Contenido: {van_parraf}')\n",
    "\n",
    "                    vanguardia['Articles'].append({\n",
    "                        'Redir':van_redir,\n",
    "                        'Title':van_title,\n",
    "                        'Author':van_autor,\n",
    "                        'Link':van_link,\n",
    "                        'Content':van_parraf,\n",
    "                        'Date':van_fecha,\n",
    "                        'Img':van_imagen\n",
    "                    })\n",
    "\n",
    "                    #print(vanguardia)\n",
    "\n",
    "                    with open('vanguardia.json','w') as f:\n",
    "                        json.dump(vanguardia, f, indent=4)\n",
    "                    \n",
    "    print('Archivo de La vanguardia listo')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    gizmo()\n",
    "    europaPress()\n",
    "    laVanguardia()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
