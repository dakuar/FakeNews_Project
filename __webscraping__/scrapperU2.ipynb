{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'termcolor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-45fb8d634515>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtermcolor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcolored\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtimeit\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'termcolor'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from termcolor import colored\n",
    "from timeit import timeit\n",
    "import threading\n",
    "import json\n",
    "import pprint\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "\n",
    "def current_date(date):\n",
    "    months = (\"Enero\", \"Febrero\", \"Marzo\", \"Abri\", \"Mayo\", \"Junio\", \"Julio\", \"Agosto\", \"Septiembre\", \"Octubre\", \"Noviembre\", \"Diciembre\")\n",
    "    day = date.day\n",
    "    month = months[date.month - 1]\n",
    "    year = date.year\n",
    "    messsage = \"{} de {} del {}\".format(day, month, year)\n",
    "\n",
    "    return messsage\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "\n",
    "#driver = webdriver.Chrome(\"./chromedriver.exe\")\n",
    "#driver.get(\"https://www.critica.com.pa/mundo\")\n",
    "gizmodo = {} #DICCIONARIO\n",
    "gizmodo['Articles'] = []\n",
    "europa_press = {}\n",
    "europa_press['Articles'] = []\n",
    "vanguardia = {} #DICCIONARIO\n",
    "vanguardia['Articles'] = []\n",
    "todayMund = {}\n",
    "todayMund['Articles'] = []\n",
    "\n",
    "URL_ALMIN = 'https://www.lavanguardia.com/alminuto'\n",
    "URL = 'https://es.gizmodo.com/c/tecnologia'\n",
    "URL_EUROPA = 'https://www.europapress.es/internacional/'\n",
    "URL_MUNDOTODAY ='https://www.elmundotoday.com/'\n",
    "      \n",
    "def content_page(link):\n",
    "    link_cont = requests.get(link)\n",
    "    soup = BeautifulSoup(link_cont.text, 'html.parser')\n",
    "    container = soup.find_all('p',{'class':'sc-77igqf-0 dQFbxq'})\n",
    "    parraf = []\n",
    "    for parrafo in container:\n",
    "        parrafo\n",
    "        parraf.append(parrafo.getText())\n",
    "    return(parraf)\n",
    "    #return parraf\n",
    "'''\n",
    "class Scrapy:\n",
    "    def __ini__(self,title,author,content,date,urlLink,cont_link,img):\n",
    "        self.title = title\n",
    "        self.author = author\n",
    "        self.content = content\n",
    "        self.date = date\n",
    "        self.urlLink = urlLink\n",
    "        self.cont_link = cont_link\n",
    "        self.img = img\n",
    "        \n",
    "    def __str__(self):\n",
    "        return str(self.__class__) + \": \" + str(self.__dict__)\n",
    "'''\n",
    "    \n",
    "\n",
    "\n",
    "           \n",
    "def gizmo():\n",
    "    inicio_giz = time.time()\n",
    "    re = requests.get(URL)\n",
    "    codeg = re.status_code\n",
    "    print(f'Status Gizmodo: ',colored(f'{codeg}','yellow'))\n",
    "    if re.status_code == 200:\n",
    "        soup = BeautifulSoup(re.text, 'html.parser')\n",
    "\n",
    "        if soup is not None:\n",
    "            articles = soup.find_all('article',{'class':'cw4lnv-0 iTueKC js_post_item'})\n",
    "            #redires = soup.find_all\n",
    "            cant_redir = len(soup.find_all('a')) #REDIRECCIONES\n",
    "            #noticias['Articles'].append({'Redir':cant_redir})\n",
    "            for article in articles:\n",
    "                #IMPRIME EL TITULO DEL ARTICULO\n",
    "                titulo = article.find('h2', {'class':'sc-759qgu-0 iSjOfj cw4lnv-6 fGbMFF'}).getText()\n",
    "                #IMPRIME EL AUTOR DEL ARTICULO\n",
    "                autor = article.find('div',{'class':'sc-1mep9y1-0 sc-1ixdk2y-0 fCQpxO sc-1rye1-3 hwANEt'}).text\n",
    "                #IMPRIME EL LINK DEL ARTICULO\n",
    "                link = article.find('a',{'class':'sc-1out364-0 hMndXN js_link'}).get('href')\n",
    "                #IMPRIME EL CONTENIDO DEL ARTICULO\n",
    "                article_content = content_page(link)\n",
    "                #IMPRIME LA FECHA DEL ARTICULO\n",
    "                fecha = article.find('div',{'class':'sc-3nbvzd-1 kpXIm'}).getText()\n",
    "                #IMPRIME EN LINK DE LA IMAGEN DEL ARTICULO\n",
    "                if(article.find('img',{'data-format':'jpg'})):\n",
    "                    imagen = article.find('img',{'data-format':'jpg'}).get('srcset')\n",
    "                else:\n",
    "                    imagen = \"No Imagen\"\n",
    "                gizmodo['Articles'].append({\n",
    "                    'Redir':cant_redir,\n",
    "                    'Title':titulo,\n",
    "                    'Author':autor,\n",
    "                    'Link':link,\n",
    "                    'Content':article_content,\n",
    "                    'Date':fecha,\n",
    "                    'Img':imagen\n",
    "                })\n",
    "\n",
    "            #print(gizmodo)\n",
    "            with open('gizmodo.json','w') as f:\n",
    "                json.dump(gizmodo, f, indent=4)\n",
    "                               \n",
    "    print(colored('Archivo de gizmodo listo','green'))\n",
    "    t = time.time() - inicio_giz\n",
    "    print(f\"Terminado en:\",colored(f\"{t:.2f}\",\"yellow\") +\" segundos\")\n",
    "#print(\"--- %s seconds ---\" % (time.time() - inicio_giz))\n",
    "    \n",
    "def europaPress():\n",
    "    inicio_europa = time.time()\n",
    "    re = requests.get(URL_EUROPA)\n",
    "    codee = re.status_code\n",
    "    print(f'Status Europapress: ',colored(f'{codee}','yellow'))\n",
    "    #print(f'Status Europapress: {codee}')\n",
    "    if re.status_code == 200:\n",
    "        soup = BeautifulSoup(re.text, 'html.parser')\n",
    "\n",
    "        if soup is not None:\n",
    "            mundo_articles = soup.find_all('article',{'class':'articulo-portada'})\n",
    "            \n",
    "            #redires = soup.find_all\n",
    "            mundo_redir = len(soup.find_all('a')) #REDIRECCIONES\n",
    "            #print(mundo_redir)\n",
    "            #noticias['Articles'].append({'Redir':cant_redir})\n",
    "            for article in mundo_articles:\n",
    "                mundo_title = article.find('h2',{'class':'articulo-titulo'}).getText()\n",
    "                #print(mundo_title)\n",
    "                mundo_fecha = article.find('time',{'class':'hora_hace'}).getText()\n",
    "                #print(mundo_fecha)\n",
    "                mundo_link = article.find('a').get('href')\n",
    "                #print(mundo_link)\n",
    "                \n",
    "                link_req = requests.get(mundo_link)\n",
    "                soup = BeautifulSoup(link_req.text, 'html.parser')\n",
    "                mundo_container = soup.find_all('div',{'id':'NoticiaPrincipal'})\n",
    "                for mundo_art in mundo_container:\n",
    "                    #mundo_autor = mundo_art.find('span',{'class':'fotoPie'})\n",
    "                    mundo_autor = mundo_art.find('div',{'class':'captionv2'})\n",
    "                    if(hasattr(mundo_autor,'text')):\n",
    "                        mundo_autor = mundo_art.find('div',{'class':'captionv2'}).text\n",
    "                        mundo_autor = mundo_autor.split(\" - \")\n",
    "                        mundo_autor = mundo_autor[1]\n",
    "                        #print(f'Author: {mundo_autor}')\n",
    "                    else:\n",
    "                        mundo_autor = mundo_art.find('div',{'class':'captionv2'})\n",
    "                        #mundo_autor = mundo_autor.split(\" - \")\n",
    "                        #print(f'Author: {mundo_autor}')\n",
    "                    mundo_imagen = mundo_art.find('img',{'id':'fotoPrincipalNoticia'})\n",
    "                    if(hasattr(mundo_imagen,'get')):\n",
    "                        mundo_imagen = mundo_art.find('img',{'id':'fotoPrincipalNoticia'}).get('src')\n",
    "                        #print(f'Imagen: {mundo_imagen}')\n",
    "                    else:\n",
    "                        mundo_imagen = mundo_art.find('img',{'id':'fotoPrincipalNoticia'})\n",
    "                        #print(f'Imagen: {mundo_imagen}')\n",
    "                        \n",
    "                    mundo_parrafos = soup.find_all('div',{'id':'CuerpoNoticiav2'})\n",
    "                    mundo_parraf = []\n",
    "                    for parra in mundo_parrafos:\n",
    "                        mundo_parraf.append(parra.getText())\n",
    "                        #print(f'Contenido: {mundo_parraf}')\n",
    "           \n",
    "                europa_press['Articles'].append({\n",
    "                    'Redir':mundo_redir,\n",
    "                    'Title':mundo_title,\n",
    "                    'Author':mundo_autor,\n",
    "                    'Link':mundo_link,\n",
    "                    'Content':mundo_parraf,\n",
    "                    'Date':mundo_fecha,\n",
    "                    'Img':mundo_imagen\n",
    "                })\n",
    "                \n",
    "                #print(europa_press)\n",
    "                \n",
    "                with open('europapress.json','w') as f:\n",
    "                    json.dump(europa_press, f, indent=4)\n",
    "    print(colored('Archivo de europapress listo','green'))\n",
    "    eu = time.time() - inicio_europa\n",
    "    print(f\"Terminado en:\",colored(f\"{eu:.2f}\",\"yellow\") +\" segundos\")\n",
    "\n",
    "    \n",
    "def laVanguardia():\n",
    "    inicio_lavan = time.time()\n",
    "    re = requests.get(URL_ALMIN)\n",
    "    codev = re.status_code\n",
    "    print(f'Status LaVanguardia: ',colored(f'{codev}','yellow'))\n",
    "    #print(f'Status LaVanguardia: {codev}')\n",
    "    if re.status_code == 200:\n",
    "        soup = BeautifulSoup(re.text, 'html.parser')\n",
    "\n",
    "        if soup is not None:\n",
    "            van_articles = soup.find_all('li',{'class':'results-item'})\n",
    "            \n",
    "            #redires = soup.find_all\n",
    "            van_redir = len(soup.find_all('a')) #REDIRECCIONES\n",
    "            #print(van_redir)\n",
    "            #noticias['Articles'].append({'Redir':cant_redir})\n",
    "            for van_article in van_articles:\n",
    "                van_title = van_article.find('h1',{'class':'story-header-title'}).getText()\n",
    "                #print(van_title)\n",
    "                van_link = van_article.find('a',{'class':'story-header-title-link'}).get('href')\n",
    "                #print(van_link)\n",
    "                #TOMA EL LINK DEL POST EN MINIATURA PARA HACER UN NUEVO REQUEST, PERO ESTA VEZ DESDE DENTRO DEL POST\n",
    "                link_vanreq = requests.get(van_link)\n",
    "                soup = BeautifulSoup(link_vanreq.text, 'html.parser')\n",
    "                van_container = soup.find_all('div',{'class':'detail__main'})\n",
    "                for van_art in van_container:\n",
    "                    #CAPTURA EL AUTOR SI ESTA DISPONIBLE LA ETIQUETA\n",
    "                    if(van_art.find('figcaption',{'class':'img-figcaption'})):\n",
    "                        van_autor = van_art.find('figcaption',{'class':'img-figcaption'}).text\n",
    "                        #print(f'Author: {van_autor}')\n",
    "                    else:\n",
    "                        van_autor = 'Sin autor'\n",
    "                        #print(f'Author: {van_autor}')\n",
    "                    #CAPTURA LA FECHA\n",
    "                    van_fecha = van_art.find('time',{'class':'d-signature__time'}).text\n",
    "                    #print(f'Fecha: {van_fecha}')\n",
    "                    #sI EXISTE LA ETIQUETA DE IMAGEN LA CAPTURA\n",
    "                    if(van_art.find('img',{'class':'img-responsive'})):\n",
    "                        van_imagen = van_art.find('img',{'class':'img-responsive'}).get('src')\n",
    "                        #print(f'Imagen: {van_imagen}')\n",
    "                    else:\n",
    "                        van_imagen = 'Imagen no disponible'\n",
    "                        #print(f'Imagen: {van_imagen}')\n",
    "                    #BUSCA EL CONTENEDOR PADRE DE LOS PARRAFOS Y CAPTURA CADA PARRAFO\n",
    "                    van_parrafos = soup.find_all('div',{'itemprop':'articleBody'})\n",
    "                    van_parraf = []\n",
    "                    for van_parra in van_parrafos:\n",
    "                        van_parraf.append(van_parra.getText())\n",
    "                        #print(f'Contenido: {van_parraf}')\n",
    "\n",
    "                    vanguardia['Articles'].append({\n",
    "                        'Redir':van_redir,\n",
    "                        'Title':van_title,\n",
    "                        'Author':van_autor,\n",
    "                        'Link':van_link,\n",
    "                        'Content':van_parraf,\n",
    "                        'Date':van_fecha,\n",
    "                        'Img':van_imagen\n",
    "                    })\n",
    "\n",
    "                    #print(vanguardia)\n",
    "\n",
    "                    with open('vanguardia.json','w') as f:\n",
    "                        json.dump(vanguardia, f, indent=4)\n",
    "                    \n",
    "    print(colored('Archivo de La vanguardia listo','green'))\n",
    "    va = time.time() - inicio_lavan\n",
    "    print(f\"Terminado en:\",colored(f\"{va:.2f}\",\"yellow\") +\" segundos\")\n",
    "\n",
    "    \n",
    "    \n",
    "def mundoToday():\n",
    "    inicio_today = time.time()\n",
    "    re = requests.get(URL_MUNDOTODAY)\n",
    "    codeDay = re.status_code\n",
    "    print(f'Status ElMundoToday: ',colored(f'{codeDay}','yellow'))\n",
    "    if re.status_code == 200:\n",
    "        soup = BeautifulSoup(re.text, 'html.parser')\n",
    "        if soup is not None:\n",
    "            today_articles = soup.find_all('div',{'class':'item-details'})\n",
    "            \n",
    "            today_redir = len(soup.find_all('a'))\n",
    "            #print(f\"Lnks: {today_redir}\")\n",
    "            for today_article in today_articles:\n",
    "#                \n",
    "                today_fecha = current_date(now)\n",
    "                #print(today_fecha)\n",
    "                today_link = today_article.find('a').get('href')\n",
    "                #print(today_link)\n",
    "                \n",
    "                \n",
    "                today_linkreq = requests.get(today_link)\n",
    "                soup = BeautifulSoup(today_linkreq.text, 'html.parser')\n",
    "                today_container = soup.find_all('div',{'id':'tdb-autoload-article'})\n",
    "                for today_art in today_container:\n",
    "                    today_title = today_art.find('h1',{'class':'tdb-title-text'}).getText()\n",
    "                    #print(today_title)\n",
    "                    today_autor = today_art.find('div',{'class':'tdb-author-name-wrap'}).text\n",
    "                    #print(today_autor)\n",
    "                    today_imagen = today_art.find('img',{'class':'entry-thumb'}).get('src')\n",
    "                    #print(today_imagen)\n",
    "                    today_parrafos = soup.select('div.tdb-block-inner > p')\n",
    "                    today_parraf = []\n",
    "                    for today_parra in today_parrafos: \n",
    "                        today_parraf.append(today_parra.getText())\n",
    "                        #print(today_parra.getText())\n",
    "                   \n",
    "                        \n",
    "                    todayMund['Articles'].append({\n",
    "                        'Redir':today_redir,\n",
    "                        'Title':today_title,\n",
    "                        'Author':today_autor,\n",
    "                        'Link':today_link,\n",
    "                        'Content':today_parraf,\n",
    "                        'Date':today_fecha,\n",
    "                        'Img':today_imagen\n",
    "                    })\n",
    "                    \n",
    "                    with open('MundoToday.json','w') as f:\n",
    "                        json.dump(todayMund, f, indent=4)\n",
    "\n",
    "    print(colored('Archivo de ELMundoToday listo','green'))\n",
    "    to = time.time() - inicio_today\n",
    "    print(f\"Terminado en:\",colored(f\"{to:.2f}\",\"yellow\") +\" segundos\")\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    threading.Thread(target=gizmo())\n",
    "    threading.Thread(target=europaPress())\n",
    "    threading.Thread(target=laVanguardia())\n",
    "    threading.Thread(target=mundoToday())\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'gizmodo.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2ef920b997ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# json.loads(string)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpprint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gizmodo.json\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'gizmodo.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pprint\n",
    "# json.loads(string)\n",
    "pprint.pprint ( json.loads(open(\"gizmodo.json\",\"r\").read() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "day = datetime.now()\n",
    "print(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def current_date_format(date):\n",
    "    months = (\"Enero\", \"Febrero\", \"Marzo\", \"Abri\", \"Mayo\", \"Junio\", \"Julio\", \"Agosto\", \"Septiembre\", \"Octubre\", \"Noviembre\", \"Diciembre\")\n",
    "    day = date.day\n",
    "    month = months[date.month - 1]\n",
    "    year = date.year\n",
    "    messsage = \"{} de {} del {}\".format(day, month, year)\n",
    "\n",
    "    return messsage\n",
    "\n",
    "now = datetime.now()\n",
    "print(current_date_format(now))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
